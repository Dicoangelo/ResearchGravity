<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=0:1a1a2e,100:00d9ff&height=200&section=header&text=ResearchGravity&fontSize=50&fontColor=ffffff&animation=fadeIn&fontAlignY=35&desc=Metaventions%20AI%20Research%20Framework&descSize=20&descAlignY=55" />
</p>

<p align="center">
  <strong>Frontier intelligence for meta-invention. Research that compounds.</strong>
</p>

<p align="center">
  <em>"Let the invention be hidden in your vision"</em>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Version-6.1.0-00d9ff?style=for-the-badge" alt="Version" />
  <img src="https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python" />
  <img src="https://img.shields.io/badge/License-MIT-green?style=for-the-badge" alt="License" />
  <img src="https://img.shields.io/badge/Status-Production-success?style=for-the-badge" alt="Status" />
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Sessions-114+-4a0080?style=for-the-badge" alt="Sessions" />
  <img src="https://img.shields.io/badge/Findings-2,530+-00d9ff?style=for-the-badge" alt="Findings" />
  <img src="https://img.shields.io/badge/URLs-8,935+-1a1a2e?style=for-the-badge" alt="URLs" />
  <img src="https://img.shields.io/badge/Tokens-27M+-success?style=for-the-badge" alt="Tokens" />
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Graph_Nodes-11,579-purple?style=for-the-badge" alt="Graph Nodes" />
  <img src="https://img.shields.io/badge/API_Endpoints-25-blue?style=for-the-badge" alt="API Endpoints" />
  <img src="https://img.shields.io/badge/Critics-3-orange?style=for-the-badge" alt="Critics" />
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Metaventions_AI-Architected_Intelligence-1a1a2e?style=for-the-badge" alt="Metaventions AI" />
</p>

---

## Why â€¢ What's New â€¢ Architecture â€¢ Quick Start â€¢ Auto-Capture â€¢ Sources â€¢ Contact

---

## Proof Deck â€” See It Work

<p align="center">
  <img src="scripts/proof/proof-deck.gif" alt="ResearchGravity Proof Deck â€” 9-slide interactive demo" width="100%" />
</p>

<p align="center">
  <em>9-slide interactive proof: real DB stats, EvidencedFinding schema, 3-stream oracle critique, and a live pipeline demo that writes real findings to antigravity.db.</em>
</p>

<p align="center">
  <a href="scripts/proof/proof-deck.html"><strong>Open Interactive Deck</strong></a>
</p>

---

## What's New in v6.1 â€” Security & Reliability (January 2026)

**Production-hardened API with enterprise security.**

| Feature | Description |
|---------|-------------|
| **ğŸ” JWT Authentication** | Token-based auth with `/api/auth/token` endpoint |
| **â±ï¸ Rate Limiting** | slowapi integration (10/min search, 30/min write) |
| **ğŸ›¡ï¸ Input Validation** | Path traversal prevention, session ID sanitization |
| **ğŸ“ Structured Logging** | JSON/console formats with request context |
| **ğŸ”„ Dead-Letter Queue** | Failed writes queued for retry with exponential backoff |
| **âš¡ Async Cohere** | Non-blocking embedding calls via `asyncio.to_thread` |
| **ğŸ”’ Connection Pool** | Semaphore-guarded SQLite pool (race condition fix) |

### Authentication

```bash
# Get JWT token
curl -X POST http://localhost:3847/api/auth/token \
  -H "Content-Type: application/json" \
  -d '{"client_id": "my-app", "scope": "write"}'

# Use token
curl -H "Authorization: Bearer <token>" http://localhost:3847/api/auth/me

# Or use API key
curl -H "X-API-Key: <your-api-key>" http://localhost:3847/api/v2/stats
```

### Environment Variables

```bash
export RG_SECRET_KEY=$(python -c "import secrets; print(secrets.token_hex(32))")
export RG_API_KEY="your-service-api-key"
export RG_LOG_LEVEL="INFO"  # DEBUG, INFO, WARNING, ERROR
export RG_LOG_JSON="true"   # JSON format for production
```

---

## What's New in v6.0 â€” Interactive Research Platform (January 2026)

**From manual workflow to intelligent auto-capture.** 3x faster research sessions with real-time URL capture.

| Feature | Description |
|---------|-------------|
| **ğŸ® Interactive REPL** | Real-time research CLI with Rich terminal UI |
| **ğŸ”„ Auto-Capture V2** | Automatic URL/finding extraction from Claude sessions (+70% capture rate) |
| **ğŸ§  Intelligence Layer** | CLI + API + REPL access to meta-learning predictions |
| **ğŸ’¾ sqlite-vec Storage** | Local vector storage with FTS fallback (no external dependencies) |
| **ğŸ‘ï¸ File Watcher** | Implicit session creation from Claude activity |
| **ğŸ“Š Dual-Write Engine** | Qdrant + sqlite-vec with automatic failover |

### Interactive REPL

```bash
python3 scripts/session/repl.py

# Commands:
rg> start "multi-agent orchestration"   # Initialize session
rg> url https://arxiv.org/...           # Log URL (auto-classify)
rg> finding "Key insight about..."      # Capture finding
rg> predict                             # Session quality prediction
rg> search "consensus algorithms"       # Semantic search past sessions
rg> archive                             # Finalize session
```

### Auto-Capture V2

```bash
python3 scripts/session/auto_capture_v2.py scan         # Scan last 24 hours
python3 scripts/session/auto_capture_v2.py scan --hours 48
python3 scripts/session/auto_capture_v2.py status       # Show capture stats
```

### Intelligence CLI

```bash
python3 scripts/prediction/intelligence.py predict "task"   # Session quality prediction
python3 scripts/prediction/intelligence.py optimal-time     # Best hour for deep work
python3 scripts/prediction/intelligence.py errors "context" # Likely errors + prevention
python3 scripts/prediction/intelligence.py patterns         # Session patterns
```

### Intelligence API

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/v2/intelligence/status` | GET | System capabilities |
| `/api/v2/intelligence/predict` | POST | Unified prediction |
| `/api/v2/intelligence/patterns` | GET | Session patterns |
| `/api/v2/intelligence/errors` | POST | Likely errors |
| `/api/v2/intelligence/feedback` | POST | Outcome feedback |

### File Watcher

```bash
python3 scripts/session/watcher.py daemon   # Start as background daemon
python3 scripts/session/watcher.py status   # Check daemon status
python3 scripts/session/watcher.py stop     # Stop daemon
```

### Storage Modes

```
Priority: Qdrant â†’ sqlite-vec â†’ FTS fallback
- Qdrant: Full semantic search (requires server)
- sqlite-vec: Single-file vectors (offline capable)
- FTS: Full-text search fallback (always available)
```

### Embedding Providers (SOTA 2026)

```
Priority: Cohere v4 â†’ Cohere v3 â†’ SBERT offline

Cohere embed-v4.0 (default):
- Multimodal (text + images)
- 128k context window
- Matryoshka dimensions: 256, 512, 1024, 1536

Dimension Options:
- 1536d: Maximum quality
- 1024d: Balanced (default)
- 512d:  50% storage savings
- 256d:  83% storage savings

Fallback Chain:
- Cohere v4 â†’ Cohere v3 â†’ SBERT (all-MiniLM-L6-v2)
```

Auto-switches on API failure. No manual configuration needed.

---

## What's New in v5.0 â€” Chief of Staff (January 2026)

**The AI Second Brain is now complete.** Full infrastructure for sovereign knowledge management.

| Feature | Description |
|---------|-------------|
| **ğŸ”® Meta-Learning Engine** | Predictive session intelligence from 666+ outcomes, 1,014 cognitive states |
| **ğŸ›ï¸ Storage Triad** | SQLite (WAL mode, FTS5) + Qdrant (semantic search) |
| **âš–ï¸ Writer-Critic System** | 3 critics validate archives, evidence, and context packs |
| **ğŸ•¸ï¸ Graph Intelligence** | 11,579 nodes, 13,744 edges â€” concept relationships & lineage |
| **ğŸ”Œ REST API** | 22 endpoints on port 3847 for cross-app integration |
| **ğŸ“Š Oracle Consensus** | Multi-stream validation for high-stakes outputs |
| **ğŸ¯ Evidence Layer** | Citations, confidence scoring, source validation |

### Chief of Staff Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CHIEF OF STAFF INFRASTRUCTURE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   CAPTURE   â”‚â”€â”€â”€â–¶â”‚  STORAGE    â”‚â”€â”€â”€â–¶â”‚ INTELLIGENCEâ”‚â”€â”€â”€â–¶â”‚  RETRIEVAL  â”‚   â”‚
â”‚  â”‚             â”‚    â”‚   TRIAD     â”‚    â”‚             â”‚    â”‚     API     â”‚   â”‚
â”‚  â”‚ Sessions    â”‚    â”‚             â”‚    â”‚ Writer      â”‚    â”‚             â”‚   â”‚
â”‚  â”‚ URLs        â”‚    â”‚ SQLite      â”‚    â”‚ Critic      â”‚    â”‚ REST /api/* â”‚   â”‚
â”‚  â”‚ Findings    â”‚    â”‚ Qdrant      â”‚    â”‚ Oracle      â”‚    â”‚ Graph /v2   â”‚   â”‚
â”‚  â”‚ Transcripts â”‚    â”‚ Graph       â”‚    â”‚ Evidence    â”‚    â”‚ SDK         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                           GRAPH INTELLIGENCE                            â”‚  â”‚
â”‚  â”‚                                                                         â”‚  â”‚
â”‚  â”‚   Sessions â”€â”€containsâ”€â”€â–¶ Findings â”€â”€citesâ”€â”€â–¶ Papers                    â”‚  â”‚
â”‚  â”‚      â”‚                      â”‚                   â”‚                       â”‚  â”‚
â”‚  â”‚      â””â”€â”€â”€â”€â”€â”€enablesâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€derives_fromâ”€â”€â”€â”˜                       â”‚  â”‚
â”‚  â”‚                                                                         â”‚  â”‚
â”‚  â”‚   11,579 Nodes  â€¢  13,744 Edges  â€¢  Concept Clusters  â€¢  Lineage       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### v4.0 Features (Still Available)

| Feature | Description |
|---------|-------------|
| **ğŸ§  CPB Module** | Cognitive Precision Bridge â€” 5-path AI orchestration |
| **ğŸ¯ ELITE TIER** | 5-agent ACE consensus, Opus-first routing, 0.75 DQ bar |
| **ğŸ“Š DQ Scoring** | Validity (40%) + Specificity (30%) + Correctness (30%) |
| **ğŸ”€ Smart Routing** | Auto-select path based on query complexity |

### CPB Execution Paths

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COGNITIVE PRECISION BRIDGE (CPB)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Query â†’ [Complexity Analysis] â†’ Path Selection â†’ Execution â†’ DQ Score  â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚  DIRECT  â”‚   RLM    â”‚   ACE    â”‚  HYBRID  â”‚ CASCADE  â”‚              â”‚
â”‚  â”‚  <0.2    â”‚ 0.2-0.5  â”‚ 0.5-0.7  â”‚  >0.7+   â”‚  >0.7    â”‚              â”‚
â”‚  â”‚  Simple  â”‚ Context  â”‚ Consensusâ”‚ Combined â”‚ Full     â”‚              â”‚
â”‚  â”‚  ~1s     â”‚  ~5s     â”‚   ~5s    â”‚  ~10s    â”‚  ~15s    â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                                         â”‚
â”‚  5-Agent ACE Ensemble:                                                  â”‚
â”‚  ğŸ”¬ Analyst | ğŸ¤” Skeptic | ğŸ”„ Synthesizer | ğŸ› ï¸ Pragmatist | ğŸ”­ Visionary â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ†• CPB Precision Mode v2.0

**Research-grounded answers with 95%+ quality target.** Combines tiered search, grounded generation, and cutting-edge convergence research.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PRECISION MODE v2 PIPELINE                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Query                                                                  â”‚
â”‚    â”‚                                                                    â”‚
â”‚    â–¼ PHASE 1: TIERED SEARCH (ResearchGravity methodology)              â”‚
â”‚    â”‚  â”œâ”€â”€ Tier 1: arXiv, Labs, Industry News                           â”‚
â”‚    â”‚  â”œâ”€â”€ Tier 2: GitHub, Benchmarks, Social                           â”‚
â”‚    â”‚  â””â”€â”€ Tier 3: Internal learnings (Qdrant)                          â”‚
â”‚    â”‚                                                                    â”‚
â”‚    â–¼ PHASE 2: CONTEXT GROUNDING                                        â”‚
â”‚    â”‚  â””â”€â”€ Build citation-ready context (agents cite ONLY these)        â”‚
â”‚    â”‚                                                                    â”‚
â”‚    â–¼ PHASE 3: GROUNDED CASCADE (7 agents)                              â”‚
â”‚    â”‚  â””â”€â”€ ğŸ”¬ğŸ¤”ğŸ”„ğŸ› ï¸ğŸ”­ğŸ“šğŸ’¡ with citation enforcement                      â”‚
â”‚    â”‚                                                                    â”‚
â”‚    â–¼ PHASE 4: MAR CONSENSUS (Multi-Agent Reflexion)                    â”‚
â”‚    â”‚  â””â”€â”€ ValidityCritic + EvidenceCritic + ActionabilityCritic        â”‚
â”‚    â”‚                                                                    â”‚
â”‚    â–¼ PHASE 5: TARGETED REFINEMENT (IMPROVE pattern)                    â”‚
â”‚    â”‚  â””â”€â”€ Fix weakest DQ dimension per retry                           â”‚
â”‚    â”‚                                                                    â”‚
â”‚    â–¼ PHASE 6: EDITORIAL FRAME                                          â”‚
â”‚    â”‚  â””â”€â”€ Extract thesis / gap / innovation direction                  â”‚
â”‚    â”‚                                                                    â”‚
â”‚    â–¼ Result (DQ score + verifiable citations)                          â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Feature | Description |
|---------|-------------|
| **Tiered Search** | arXiv API + GitHub API + Internal Qdrant |
| **Time-Decay Scoring** | Research: 23-day half-life, News: 2-day |
| **Signal Quantification** | Stars, citations, dates extracted |
| **Grounded Generation** | Agents can ONLY cite retrieved sources |
| **MAR Consensus** | 3 persona critics â†’ synthesis (arXiv:2512.20845) |
| **Targeted Refinement** | IMPROVE pattern (arXiv:2502.18530) |

**Usage:**
```bash
python3 -m cpb precision "your research question" --verbose
```

### v3.5 Changelog

| Feature | Description |
|---------|-------------|
| **Precision Bridge Research** | Tesla US20260017019A1 â†’ RLM synthesis methodology |
| **Cognitive Wallet Tracking** | 114 sessions, 2,530 findings, 8,935 URLs, 27M tokens |
| **Deep Dive Workflow** | Multi-paper synthesis with implementation output |
| **Framework Extraction** | COMPRESS â†’ EXPLORE â†’ RECONSTRUCT pattern identified |

### Notable Research Sessions

| Session | Papers | Output |
|---------|--------|--------|
| Chief of Staff Architecture | 374 | Storage Triad, Graph Intelligence, Writer-Critic |
| Tesla Mixed-Precision RoPE | 15 arXiv | `recursiveLanguageModel.ts` implementation |
| Multi-Agent Orchestration | 12 arXiv | ACE/DQ Scoring in OS-App |
| CPB Integration | 8 arXiv | `cpb/` Python module |
| 160+ Papers Meta-Synthesis | 160+ | Unified research index |

## What's New in v3.4

| Feature | Description |
|---------|-------------|
| **Context Prefetcher** | `scripts/session/prefetch.py` â€” Inject relevant learnings into Claude sessions |
| **Learnings Backfill** | `scripts/backfill/backfill_learnings.py` â€” Extract learnings from all archived sessions |
| **Memory Injection** | Auto-load project context, papers, and lineage at session start |
| **Shell Integration** | `prefetch`, `prefetch-clip`, `prefetch-inject` shell commands |

### v3.3 Changelog

| Feature | Description |
|---------|-------------|
| **YouTube Research** | `scripts/importers/youtube_channel.py` â€” Channel analysis and transcript extraction |
| **Enhanced Backfill** | Improved session recovery with better transcript parsing |
| **Ecosystem Sync** | Deeper integration with Agent Core orchestration |

### v3.2 Changelog

| Feature | Description |
|---------|-------------|
| **Auto-Capture** | Sessions automatically tracked â€” URLs, findings, full transcripts extracted |
| **Lineage Tracking** | Link research sessions to implementation projects |
| **Project Registry** | 4 registered projects with cross-referenced research |
| **Context Loader** | Auto-load project context from any directory |
| **Unified Index** | Cross-reference by paper, topic, or session |
| **Backfill** | Recover research from historical Claude sessions |

---

## Why ResearchGravity?

Traditional research workflows fail at the frontier:

| Problem | Impact |
|---------|--------|
| Single-source blindspots | Missing critical signals |
| No synthesis | Raw links â‰  research |
| No session continuity | Context lost between sessions |
| No quality standard | Inconsistent output |

**ResearchGravity** solves this with:

- **Multi-tier source hierarchy** â€” Tier 1 (primary), Tier 2 (amplifiers), Tier 3 (context)
- **Cold Start Protocol** â€” Never lose session context
- **Synthesis workflow** â€” Thesis â†’ Gap â†’ Innovation Direction
- **Quality checklist** â€” Consistent Metaventions-grade output

---

## Architecture

<p align="center">
  <a href="docs/architecture-dark.png">
    <img src="docs/architecture-dark.png" alt="ResearchGravity v6.1 â€” 6-Tier Sovereign Research Intelligence System (Dark)" width="100%" />
  </a>
</p>

<details>
<summary>View light mode architecture</summary>
<p align="center">
  <a href="docs/architecture-light.png">
    <img src="docs/architecture-light.png" alt="ResearchGravity v6.1 Architecture â€” Light Mode" width="100%" />
  </a>
</p>
</details>

### Directory Structure

```
ResearchGravity/
â”‚
â”œâ”€â”€ api/                            # REST API Server (v5.0+)
â”‚   â”œâ”€â”€ server.py                   # FastAPI on port 3847 â€” 25 endpoints
â”‚   â””â”€â”€ routes/                     # API route modules
â”‚
â”œâ”€â”€ capture/                        # Event capture & normalization
â”œâ”€â”€ chrome-extension/               # Browser extension for URL capture
â”œâ”€â”€ cli/                            # CLI Package (v6.0) â€” REPL commands & UI
â”‚
â”œâ”€â”€ coherence_engine/               # Cross-platform coherence detection
â”œâ”€â”€ cpb/                            # Cognitive Precision Bridge (v4.0)
â”œâ”€â”€ critic/                         # Writer-Critic validation system (v5.0)
â”œâ”€â”€ dashboard/                      # Web dashboard UI
â”œâ”€â”€ delegation/                     # Intelligent delegation (arXiv:2602.11865)
â”‚
â”œâ”€â”€ docs/                           # All documentation
â”‚   â”œâ”€â”€ context-packs/              # Context pack design & implementation docs
â”‚   â”œâ”€â”€ meta-learning/              # Meta-learning architecture docs
â”‚   â”œâ”€â”€ phases/                     # Phase completion records
â”‚   â”œâ”€â”€ prds/                       # Product requirement documents
â”‚   â”œâ”€â”€ routing/                    # Routing workflow docs
â”‚   â””â”€â”€ ucw/                        # UCW whitepaper & cognitive profile
â”‚
â”œâ”€â”€ graph/                          # Graph Intelligence (v5.0) â€” 11K nodes
â”œâ”€â”€ mcp_raw/                        # MCP protocol & embeddings layer
â”œâ”€â”€ methods/                        # Research methodology definitions
â”œâ”€â”€ notebooklm_mcp/                 # NotebookLM MCP server (37 tools)
â”‚
â”œâ”€â”€ scripts/                        # All utility scripts
â”‚   â”œâ”€â”€ backfill/                   # Backfill & migration (9 scripts)
â”‚   â”œâ”€â”€ coherence/                  # Coherence analysis pipeline
â”‚   â”œâ”€â”€ context-packs/              # Context pack build/select/metrics
â”‚   â”œâ”€â”€ evidence/                   # Evidence extraction & validation
â”‚   â”œâ”€â”€ importers/                  # Platform importers (ChatGPT, Grok, CLI)
â”‚   â”œâ”€â”€ prediction/                 # Intelligence & prediction engine
â”‚   â”œâ”€â”€ proof/                      # Demo proof & interactive deck
â”‚   â”œâ”€â”€ routing/                    # Routing metrics & research sync
â”‚   â”œâ”€â”€ session/                    # Session management (status, init, REPL)
â”‚   â””â”€â”€ visual/                     # Visual generation scripts
â”‚
â”œâ”€â”€ storage/                        # Storage Engine â€” SQLite + Qdrant + sqlite-vec
â”œâ”€â”€ tests/                          # All test files
â”œâ”€â”€ ucw/                            # Universal Cognitive Wallet
â”œâ”€â”€ webhook/                        # Webhook event receiver
â”‚
â”œâ”€â”€ mcp_server.py                   # MCP server entry point
â”œâ”€â”€ setup.sh                        # Bootstrap script
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ ruff.toml                       # Linter config
```

---

## CPB Module (v4.0)

The **Cognitive Precision Bridge** provides precision-aware AI orchestration.

### Quick Start

```python
from cpb import cpb, analyze, score_response

# Analyze query complexity
result = analyze("Design a distributed cache system")
print(f"Complexity: {result['complexity_score']:.2f}")
print(f"Path: {result['selected_path']}")

# Build ACE consensus prompts (5 agents)
prompts = cpb.build_ace_prompts("What's the best auth strategy?")
for p in prompts:
    print(f"[{p['agent']}] {p['system_prompt'][:50]}...")

# Score response quality
dq = score_response(query, response)
print(f"DQ: {dq.overall:.2f} (V:{dq.validity:.2f} S:{dq.specificity:.2f} C:{dq.correctness:.2f})")
```

### CLI Commands

```bash
# Analyze query complexity
python3 -m cpb.cli analyze "Your query here"

# Score a response
python3 -m cpb.cli score --query "Q" --response "R"

# View DQ statistics
python3 -m cpb.cli stats --days 30

# Check CPB status
python3 -m cpb.cli status

# Via routing-metrics
python3 scripts/routing/routing-metrics.py cpb analyze "Your query"
python3 scripts/routing/routing-metrics.py cpb status
```

### ELITE TIER Configuration

| Setting | Value | Description |
|---------|-------|-------------|
| Complexity Thresholds | 0.2 / 0.5 | Lower = more orchestration |
| ACE Agent Count | 5 | Full ensemble |
| DQ Quality Bar | 0.75 | Higher standard |
| Default Path | cascade | Full pipeline |
| RLM Iterations | 25 | Deeper decomposition |
| Model Routing | Opus-first | Maximum quality |

### 5-Agent ACE Ensemble

| Agent | Role | Focus |
|-------|------|-------|
| ğŸ”¬ **Analyst** | Evidence evaluator | Data, logic, consistency |
| ğŸ¤” **Skeptic** | Challenge assumptions | Failure modes, risks |
| ğŸ”„ **Synthesizer** | Pattern finder | Connections, frameworks |
| ğŸ› ï¸ **Pragmatist** | Feasibility checker | Implementation, constraints |
| ğŸ”­ **Visionary** | Strategic thinker | Long-term, second-order effects |

### Research Foundation

- **arXiv:2512.24601** (RLM) - Recursive context externalization
- **arXiv:2511.15755** (DQ) - Decisional quality measurement
- **arXiv:2508.17536** - Voting vs Debate consensus strategies

---

## Installation

### Prerequisites

- Python 3.8+
- pip or pipenv

### Setup

```bash
# Clone the repository
git clone https://github.com/Dicoangelo/ResearchGravity.git
cd ResearchGravity

# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### Configuration

Create `~/.agent-core/config.json` for API keys:

```json
{
  "cohere": {
    "api_key": "your-cohere-api-key"
  }
}
```

Or use environment variables:

```bash
export COHERE_API_KEY="your-cohere-api-key"
```

**Optional (for API server):**

```bash
export RG_SECRET_KEY=$(python -c "import secrets; print(secrets.token_hex(32))")
export RG_API_KEY="your-service-api-key"
```

### Verify Installation

```bash
python3 scripts/session/status.py
```

---

## Quick Start

### 1. Check Session State
```bash
python3 scripts/session/status.py
```

### 2. Initialize New Session
```bash
# Basic session
python3 scripts/session/init_session.py "your research topic"

# Pre-link to implementation project (v3.1)
python3 scripts/session/init_session.py "multi-agent consensus" --impl-project os-app
```

### 3. Research & Log URLs
```bash
# Log a Tier 1 research paper
python3 scripts/session/log_url.py https://arxiv.org/abs/2601.05918 \
  --tier 1 --category research --relevance 5 --used

# Log industry news
python3 scripts/session/log_url.py https://techcrunch.com/... \
  --tier 1 --category industry --relevance 4 --used
```

### 4. Archive When Complete
```bash
python3 scripts/session/archive_session.py
```

### 5. Check Tracker Status (v3.1)
```bash
python3 scripts/session/session_tracker.py status
```

### 6. Load Project Context (v3.2)
```bash
# Auto-detect from current directory
python3 scripts/session/project_context.py

# List all projects
python3 scripts/session/project_context.py --list

# View unified index
python3 scripts/session/project_context.py --index
```

---

## Research Workflow

### Phase 1: Signal Capture (30 min)
```
1. Scan Tier 1 sources for last 24-48 hours
2. Log ALL URLs (used or not) via log_url.py
3. Tag each with: tier, category, relevance (1-5)
```

### Phase 2: Synthesis (20 min)
```
1. Group findings by theme (not source)
2. Identify the GAP â€” what's missing?
3. Draft thesis: "X is happening because Y, which means Z"
```

### Phase 3: Editorial Frame (10 min)
```
1. Write 1-paragraph summary with thesis
2. Each finding: [Name](URL) + signal + rationale
3. End with: "Innovation opportunity: ..."
```

---

## Source Hierarchy

### Tier 1: Primary Sources (Check Daily)

| Category | Sources |
|----------|---------|
| **Research** | arXiv (cs.AI, cs.SE, cs.LG), HuggingFace Papers |
| **Labs** | OpenAI, Anthropic, Google AI, Meta AI, DeepMind |
| **Industry** | TechCrunch, The Verge, Ars Technica, Wired |

### Tier 2: Signal Amplifiers

| Category | Sources |
|----------|---------|
| **GitHub** | Trending, Topics, Releases |
| **Benchmarks** | METR, ARC Prize, LMSYS, PapersWithCode |
| **Social** | X/Twitter key accounts, HN, Reddit ML |

### Tier 3: Deep Context

| Category | Sources |
|----------|---------|
| **Newsletters** | Import AI, The Batch, Latent Space |
| **Forums** | LessWrong, Alignment Forum |

---

## Quality Checklist

Before archiving a session, verify:

- [ ] Scanned all Tier 1 sources for timeframe
- [ ] Logged 10+ URLs minimum
- [ ] Identified at least one GAP
- [ ] Wrote thesis statement
- [ ] Each finding has: link + signal + rationale
- [ ] Innovation direction is concrete, not vague

---

## Cold Start Protocol

When invoking ResearchGravity, always run `scripts/session/status.py` first:

```
==================================================
  ResearchGravity â€” Metaventions AI
==================================================

ğŸ“ ACTIVE SESSION
   Topic: [current topic]
   URLs logged: X | Findings: Y | Thesis: Yes/No

ğŸ“š RECENT SESSIONS
   1. [topic] â€” [date]
   2. [topic] â€” [date]

--------------------------------------------------
OPTIONS:
  â†’ Continue active session
  â†’ Resume archived session
  â†’ Start fresh
--------------------------------------------------
```

---

## Auto-Capture & Lineage (v3.1)

**All research sessions are now automatically tracked.** No more lost research.

### What Gets Captured

| Artifact | Storage |
|----------|---------|
| Full transcript | `~/.agent-core/sessions/[id]/full_transcript.txt` |
| All URLs | `urls_captured.json` |
| Key findings | `findings_captured.json` |
| Cross-project links | `lineage.json` |

### Lineage Tracking

Link research sessions to implementation projects:

```bash
# Pre-link at session start
python3 scripts/session/init_session.py "multi-agent DQ" --impl-project os-app

# Manual link after research
python3 scripts/session/session_tracker.py link [session-id] [project]
```

### Backfill Historical Sessions

Recover research from old Claude sessions:

```bash
# Scan recent history
python3 scripts/session/auto_capture.py scan --hours 48

# Backfill specific session
python3 scripts/session/auto_capture.py backfill ~/.claude/projects/.../session.jsonl --topic "..."
```

---

## Context Prefetcher (v3.4)

**Memory injection for Claude sessions.** Automatically load relevant learnings, project memory, and research papers at session start.

### Basic Usage

```bash
# Auto-detect project from current directory
python3 scripts/session/prefetch.py

# Specific project with papers
python3 scripts/session/prefetch.py --project os-app --papers

# Filter by topic
python3 scripts/session/prefetch.py --topic multi-agent --days 30

# Copy to clipboard
python3 scripts/session/prefetch.py --project os-app --clipboard

# Inject into ~/CLAUDE.md
python3 scripts/session/prefetch.py --project os-app --inject
```

### Shell Commands

After sourcing `~/.claude/scripts/auto-context.sh`:

```bash
prefetch                    # Auto-detect project, last 14 days
prefetch os-app 7           # Specific project, last 7 days
prefetch-clip               # Copy context to clipboard
prefetch-inject             # Inject into ~/CLAUDE.md
prefetch-topic "consensus"  # Filter by topic across all sessions
backfill-learnings          # Regenerate learnings.md from all sessions
```

### CLI Options

| Flag | Description |
|------|-------------|
| `--project, -p` | Project ID to load context for |
| `--topic, -t` | Filter by topic keywords |
| `--days, -d` | Limit to last N days (default: 14) |
| `--limit, -l` | Max learning entries (default: 5) |
| `--papers` | Include relevant arXiv papers |
| `--clipboard, -c` | Copy to clipboard (macOS) |
| `--inject, -i` | Inject into ~/CLAUDE.md |
| `--json` | Output as JSON |
| `--quiet, -q` | Suppress info output |

### Backfill Learnings

Extract learnings from all archived sessions:

```bash
# Process all sessions
python3 scripts/backfill/backfill_learnings.py

# Last 7 days only
python3 scripts/backfill/backfill_learnings.py --since 7

# Specific session
python3 scripts/backfill/backfill_learnings.py --session <session-id>

# Preview without writing
python3 scripts/backfill/backfill_learnings.py --dry-run
```

### What Gets Injected

| Component | Source |
|-----------|--------|
| Project info | `projects.json` â€” name, focus, tech stack, status |
| Project memory | `memory/projects/[project].md` |
| Recent learnings | `memory/learnings.md` â€” filtered by project/topic/days |
| Research papers | `paper_index` in projects.json |
| Lineage | Research sessions â†’ features implemented |

---

## Integration

ResearchGravity integrates with the **Antigravity ecosystem**:

| Environment | Use Case |
|-------------|----------|
| **CLI** (Claude Code) | Planning, parallel sessions, synthesis |
| **Antigravity** (VSCode) | Coding, preview, browser research |
| **Web** (claude.ai) | Handoff, visual review |

---

## API Server (v5.0)

Start the Chief of Staff API:

```bash
python api/server.py
# Running on http://127.0.0.1:3847
```

### Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/v1/sessions` | GET | List all sessions |
| `/api/v1/sessions/{id}` | GET | Get session details |
| `/api/v1/findings` | GET | Search findings |
| `/api/v1/urls` | GET | Search URLs |
| `/api/v2/graph/stats` | GET | Graph statistics |
| `/api/v2/graph/session/{id}` | GET | Session subgraph (D3 format) |
| `/api/v2/graph/related/{id}` | GET | Related sessions |
| `/api/v2/graph/lineage/{id}` | GET | Research lineage chain |
| `/api/v2/graph/clusters` | GET | Concept clusters |
| `/api/v2/graph/timeline` | GET | Research timeline |
| `/api/v2/graph/network/{id}` | GET | Concept network |
| **`/api/v2/predict/session`** | **POST** | **Predict session outcome, quality, optimal time** |
| **`/api/v2/predict/errors`** | **POST** | **Predict potential errors with solutions** |
| **`/api/v2/predict/optimal-time`** | **POST** | **Suggest best time to work on task** |

### Example Queries

```bash
# Get graph stats
curl http://localhost:3847/api/v2/graph/stats | jq

# Get session subgraph
curl "http://localhost:3847/api/v2/graph/session/my-session-id?depth=2" | jq

# Find concept clusters
curl "http://localhost:3847/api/v2/graph/clusters?min_size=5" | jq

# Predict session outcome (Meta-Learning Engine)
curl -X POST http://localhost:3847/api/v2/predict/session \
  -H "Content-Type: application/json" \
  -d '{"intent": "implement authentication system", "track_prediction": false}' | jq

# Predict potential errors
curl -X POST http://localhost:3847/api/v2/predict/errors \
  -H "Content-Type: application/json" \
  -d '{"intent": "git commit and push", "include_preventable_only": true}' | jq

# Get optimal work time
curl -X POST http://localhost:3847/api/v2/predict/optimal-time \
  -H "Content-Type: application/json" \
  -d '{"intent": "deep architecture work"}' | jq
```

---

## Writer-Critic System (v5.0)

High-stakes outputs are validated by dual-agent critic system:

| Critic | Target | Confidence |
|--------|--------|------------|
| **ArchiveCritic** | Archive completeness (files, metadata, findings) | 96.3% |
| **EvidenceCritic** | Citation accuracy, source validation | Threshold: 0.7 |
| **PackCritic** | Context pack relevance, token efficiency | Threshold: 0.7 |

```python
from critic import ArchiveCritic, EvidenceCritic

# Validate an archive
critic = ArchiveCritic()
result = await critic.validate("session-id")
print(f"Valid: {result.valid}, Confidence: {result.confidence:.2%}")
```

---

## Graph Intelligence (v5.0)

Query the knowledge graph:

```python
from graph import ConceptGraph, get_research_lineage

# Get session subgraph
graph = ConceptGraph()
await graph.load()
subgraph = await graph.get_session_graph("session-id", depth=2)
d3_data = subgraph.to_d3_format()  # For visualization

# Get research lineage
lineage = await get_research_lineage("session-id")
print(f"Ancestors: {len(lineage['ancestors'])}")
print(f"Descendants: {len(lineage['descendants'])}")

# Find concept clusters
clusters = await graph.get_concept_clusters(min_size=5)
```

---

## Roadmap

### Completed âœ…
- [x] Auto-capture sessions (v3.1)
- [x] Cross-project lineage tracking (v3.1)
- [x] Project registry & context loader (v3.2)
- [x] Unified research index (v3.2)
- [x] Context prefetcher & memory injection (v3.4)
- [x] Learnings backfill from archived sessions (v3.4)
- [x] CPB â€” Cognitive Precision Bridge (v4.0)
- [x] Storage Triad â€” SQLite + Qdrant (v5.0)
- [x] Writer-Critic validation system (v5.0)
- [x] Graph Intelligence â€” concept relationships (v5.0)
- [x] REST API â€” 19 endpoints (v5.0)
- [x] Evidence Layer â€” citations & confidence (v5.0)
- [x] CCC Dashboard sync (v5.0)
- [x] Interactive REPL â€” real-time research CLI (v6.0)
- [x] Auto-Capture V2 â€” +70% URL capture rate (v6.0)
- [x] Intelligence Layer â€” CLI + API + REPL (v6.0)
- [x] sqlite-vec storage â€” offline vector search (v6.0)
- [x] File Watcher â€” implicit session creation (v6.0)
- [x] Dual-Write Engine â€” Qdrant + sqlite-vec failover (v6.0)

### Future
- [ ] OS-App SDK integration
- [ ] Real-time WebSocket updates
- [ ] Browser extension for URL capture
- [ ] Team collaboration features

---

## License

MIT License â€” See [LICENSE](LICENSE)

---

## Contact

**Metaventions AI**
Dico Angelo
dicoangelo@metaventionsai.com

<p align="center">
  <a href="https://metaventionsai.com">
    <img src="https://img.shields.io/badge/Metaventions_AI-Website-00d9ff?style=for-the-badge" alt="Website" />
  </a>
  <a href="https://github.com/Dicoangelo">
    <img src="https://img.shields.io/badge/GitHub-Dicoangelo-1a1a2e?style=for-the-badge&logo=github" alt="GitHub" />
  </a>
</p>

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=0:1a1a2e,100:00d9ff&height=100&section=footer" />
</p>
