<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=0:1a1a2e,100:00d9ff&height=200&section=header&text=ResearchGravity&fontSize=50&fontColor=ffffff&animation=fadeIn&fontAlignY=35&desc=Metaventions%20AI%20Research%20Framework&descSize=20&descAlignY=55" />
</p>

<p align="center">
  <strong>Frontier intelligence for meta-invention. Research that compounds.</strong>
</p>

<p align="center">
  <em>"Let the invention be hidden in your vision"</em>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Version-6.1.0-00d9ff?style=for-the-badge" alt="Version" />
  <img src="https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python" />
  <img src="https://img.shields.io/badge/License-MIT-green?style=for-the-badge" alt="License" />
  <img src="https://img.shields.io/badge/Status-Production-success?style=for-the-badge" alt="Status" />
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Sessions-114+-4a0080?style=for-the-badge" alt="Sessions" />
  <img src="https://img.shields.io/badge/Findings-2,530+-00d9ff?style=for-the-badge" alt="Findings" />
  <img src="https://img.shields.io/badge/URLs-8,935+-1a1a2e?style=for-the-badge" alt="URLs" />
  <img src="https://img.shields.io/badge/Tokens-27M+-success?style=for-the-badge" alt="Tokens" />
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Graph_Nodes-11,579-purple?style=for-the-badge" alt="Graph Nodes" />
  <img src="https://img.shields.io/badge/API_Endpoints-25-blue?style=for-the-badge" alt="API Endpoints" />
  <img src="https://img.shields.io/badge/Critics-3-orange?style=for-the-badge" alt="Critics" />
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Metaventions_AI-Architected_Intelligence-1a1a2e?style=for-the-badge" alt="Metaventions AI" />
</p>

---

## Why â€¢ What's New â€¢ Architecture â€¢ Quick Start â€¢ Auto-Capture â€¢ Sources â€¢ Contact

---

## Proof Deck â€” See It Work

<p align="center">
  <img src="proof-deck.gif" alt="ResearchGravity Proof Deck â€” 9-slide interactive demo" width="100%" />
</p>

<p align="center">
  <em>9-slide interactive proof: real DB stats, EvidencedFinding schema, 3-stream oracle critique, and a live pipeline demo that writes real findings to antigravity.db.</em>
</p>

<p align="center">
  <a href="proof-deck.html"><strong>Open Interactive Deck</strong></a>
</p>

---

## What's New in v6.1 â€” Security & Reliability (January 2026)

**Production-hardened API with enterprise security.**

| Feature | Description |
|---------|-------------|
| **ğŸ” JWT Authentication** | Token-based auth with `/api/auth/token` endpoint |
| **â±ï¸ Rate Limiting** | slowapi integration (10/min search, 30/min write) |
| **ğŸ›¡ï¸ Input Validation** | Path traversal prevention, session ID sanitization |
| **ğŸ“ Structured Logging** | JSON/console formats with request context |
| **ğŸ”„ Dead-Letter Queue** | Failed writes queued for retry with exponential backoff |
| **âš¡ Async Cohere** | Non-blocking embedding calls via `asyncio.to_thread` |
| **ğŸ”’ Connection Pool** | Semaphore-guarded SQLite pool (race condition fix) |

### Authentication

```bash
# Get JWT token
curl -X POST http://localhost:3847/api/auth/token \
  -H "Content-Type: application/json" \
  -d '{"client_id": "my-app", "scope": "write"}'

# Use token
curl -H "Authorization: Bearer <token>" http://localhost:3847/api/auth/me

# Or use API key
curl -H "X-API-Key: <your-api-key>" http://localhost:3847/api/v2/stats
```

### Environment Variables

```bash
export RG_SECRET_KEY=$(python -c "import secrets; print(secrets.token_hex(32))")
export RG_API_KEY="your-service-api-key"
export RG_LOG_LEVEL="INFO"  # DEBUG, INFO, WARNING, ERROR
export RG_LOG_JSON="true"   # JSON format for production
```

---

## What's New in v6.0 â€” Interactive Research Platform (January 2026)

**From manual workflow to intelligent auto-capture.** 3x faster research sessions with real-time URL capture.

| Feature | Description |
|---------|-------------|
| **ğŸ® Interactive REPL** | Real-time research CLI with Rich terminal UI |
| **ğŸ”„ Auto-Capture V2** | Automatic URL/finding extraction from Claude sessions (+70% capture rate) |
| **ğŸ§  Intelligence Layer** | CLI + API + REPL access to meta-learning predictions |
| **ğŸ’¾ sqlite-vec Storage** | Local vector storage with FTS fallback (no external dependencies) |
| **ğŸ‘ï¸ File Watcher** | Implicit session creation from Claude activity |
| **ğŸ“Š Dual-Write Engine** | Qdrant + sqlite-vec with automatic failover |

### Interactive REPL

```bash
python3 repl.py

# Commands:
rg> start "multi-agent orchestration"   # Initialize session
rg> url https://arxiv.org/...           # Log URL (auto-classify)
rg> finding "Key insight about..."      # Capture finding
rg> predict                             # Session quality prediction
rg> search "consensus algorithms"       # Semantic search past sessions
rg> archive                             # Finalize session
```

### Auto-Capture V2

```bash
python3 auto_capture_v2.py scan         # Scan last 24 hours
python3 auto_capture_v2.py scan --hours 48
python3 auto_capture_v2.py status       # Show capture stats
```

### Intelligence CLI

```bash
python3 intelligence.py predict "task"   # Session quality prediction
python3 intelligence.py optimal-time     # Best hour for deep work
python3 intelligence.py errors "context" # Likely errors + prevention
python3 intelligence.py patterns         # Session patterns
```

### Intelligence API

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/v2/intelligence/status` | GET | System capabilities |
| `/api/v2/intelligence/predict` | POST | Unified prediction |
| `/api/v2/intelligence/patterns` | GET | Session patterns |
| `/api/v2/intelligence/errors` | POST | Likely errors |
| `/api/v2/intelligence/feedback` | POST | Outcome feedback |

### File Watcher

```bash
python3 watcher.py daemon   # Start as background daemon
python3 watcher.py status   # Check daemon status
python3 watcher.py stop     # Stop daemon
```

### Storage Modes

```
Priority: Qdrant â†’ sqlite-vec â†’ FTS fallback
- Qdrant: Full semantic search (requires server)
- sqlite-vec: Single-file vectors (offline capable)
- FTS: Full-text search fallback (always available)
```

### Embedding Providers (SOTA 2026)

```
Priority: Cohere v4 â†’ Cohere v3 â†’ SBERT offline

Cohere embed-v4.0 (default):
- Multimodal (text + images)
- 128k context window
- Matryoshka dimensions: 256, 512, 1024, 1536

Dimension Options:
- 1536d: Maximum quality
- 1024d: Balanced (default)
- 512d:  50% storage savings
- 256d:  83% storage savings

Fallback Chain:
- Cohere v4 â†’ Cohere v3 â†’ SBERT (all-MiniLM-L6-v2)
```

Auto-switches on API failure. No manual configuration needed.

---

## What's New in v5.0 â€” Chief of Staff (January 2026)

**The AI Second Brain is now complete.** Full infrastructure for sovereign knowledge management.

| Feature | Description |
|---------|-------------|
| **ğŸ”® Meta-Learning Engine** | Predictive session intelligence from 666+ outcomes, 1,014 cognitive states |
| **ğŸ›ï¸ Storage Triad** | SQLite (WAL mode, FTS5) + Qdrant (semantic search) |
| **âš–ï¸ Writer-Critic System** | 3 critics validate archives, evidence, and context packs |
| **ğŸ•¸ï¸ Graph Intelligence** | 11,579 nodes, 13,744 edges â€” concept relationships & lineage |
| **ğŸ”Œ REST API** | 22 endpoints on port 3847 for cross-app integration |
| **ğŸ“Š Oracle Consensus** | Multi-stream validation for high-stakes outputs |
| **ğŸ¯ Evidence Layer** | Citations, confidence scoring, source validation |

### Chief of Staff Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CHIEF OF STAFF INFRASTRUCTURE                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   CAPTURE   â”‚â”€â”€â”€â–¶â”‚  STORAGE    â”‚â”€â”€â”€â–¶â”‚ INTELLIGENCEâ”‚â”€â”€â”€â–¶â”‚  RETRIEVAL  â”‚   â”‚
â”‚  â”‚             â”‚    â”‚   TRIAD     â”‚    â”‚             â”‚    â”‚     API     â”‚   â”‚
â”‚  â”‚ Sessions    â”‚    â”‚             â”‚    â”‚ Writer      â”‚    â”‚             â”‚   â”‚
â”‚  â”‚ URLs        â”‚    â”‚ SQLite      â”‚    â”‚ Critic      â”‚    â”‚ REST /api/* â”‚   â”‚
â”‚  â”‚ Findings    â”‚    â”‚ Qdrant      â”‚    â”‚ Oracle      â”‚    â”‚ Graph /v2   â”‚   â”‚
â”‚  â”‚ Transcripts â”‚    â”‚ Graph       â”‚    â”‚ Evidence    â”‚    â”‚ SDK         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                           GRAPH INTELLIGENCE                            â”‚  â”‚
â”‚  â”‚                                                                         â”‚  â”‚
â”‚  â”‚   Sessions â”€â”€containsâ”€â”€â–¶ Findings â”€â”€citesâ”€â”€â–¶ Papers                    â”‚  â”‚
â”‚  â”‚      â”‚                      â”‚                   â”‚                       â”‚  â”‚
â”‚  â”‚      â””â”€â”€â”€â”€â”€â”€enablesâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€derives_fromâ”€â”€â”€â”˜                       â”‚  â”‚
â”‚  â”‚                                                                         â”‚  â”‚
â”‚  â”‚   11,579 Nodes  â€¢  13,744 Edges  â€¢  Concept Clusters  â€¢  Lineage       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### v4.0 Features (Still Available)

| Feature | Description |
|---------|-------------|
| **ğŸ§  CPB Module** | Cognitive Precision Bridge â€” 5-path AI orchestration |
| **ğŸ¯ ELITE TIER** | 5-agent ACE consensus, Opus-first routing, 0.75 DQ bar |
| **ğŸ“Š DQ Scoring** | Validity (40%) + Specificity (30%) + Correctness (30%) |
| **ğŸ”€ Smart Routing** | Auto-select path based on query complexity |

### CPB Execution Paths

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COGNITIVE PRECISION BRIDGE (CPB)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Query â†’ [Complexity Analysis] â†’ Path Selection â†’ Execution â†’ DQ Score  â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚  DIRECT  â”‚   RLM    â”‚   ACE    â”‚  HYBRID  â”‚ CASCADE  â”‚              â”‚
â”‚  â”‚  <0.2    â”‚ 0.2-0.5  â”‚ 0.5-0.7  â”‚  >0.7+   â”‚  >0.7    â”‚              â”‚
â”‚  â”‚  Simple  â”‚ Context  â”‚ Consensusâ”‚ Combined â”‚ Full     â”‚              â”‚
â”‚  â”‚  ~1s     â”‚  ~5s     â”‚   ~5s    â”‚  ~10s    â”‚  ~15s    â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                                         â”‚
â”‚  5-Agent ACE Ensemble:                                                  â”‚
â”‚  ğŸ”¬ Analyst | ğŸ¤” Skeptic | ğŸ”„ Synthesizer | ğŸ› ï¸ Pragmatist | ğŸ”­ Visionary â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ†• CPB Precision Mode v2.0

**Research-grounded answers with 95%+ quality target.** Combines tiered search, grounded generation, and cutting-edge convergence research.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PRECISION MODE v2 PIPELINE                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  Query                                                                  â”‚
â”‚    â”‚                                                                    â”‚
â”‚    â–¼ PHASE 1: TIERED SEARCH (ResearchGravity methodology)              â”‚
â”‚    â”‚  â”œâ”€â”€ Tier 1: arXiv, Labs, Industry News                           â”‚
â”‚    â”‚  â”œâ”€â”€ Tier 2: GitHub, Benchmarks, Social                           â”‚
â”‚    â”‚  â””â”€â”€ Tier 3: Internal learnings (Qdrant)                          â”‚
â”‚    â”‚                                                                    â”‚
â”‚    â–¼ PHASE 2: CONTEXT GROUNDING                                        â”‚
â”‚    â”‚  â””â”€â”€ Build citation-ready context (agents cite ONLY these)        â”‚
â”‚    â”‚                                                                    â”‚
â”‚    â–¼ PHASE 3: GROUNDED CASCADE (7 agents)                              â”‚
â”‚    â”‚  â””â”€â”€ ğŸ”¬ğŸ¤”ğŸ”„ğŸ› ï¸ğŸ”­ğŸ“šğŸ’¡ with citation enforcement                      â”‚
â”‚    â”‚                                                                    â”‚
â”‚    â–¼ PHASE 4: MAR CONSENSUS (Multi-Agent Reflexion)                    â”‚
â”‚    â”‚  â””â”€â”€ ValidityCritic + EvidenceCritic + ActionabilityCritic        â”‚
â”‚    â”‚                                                                    â”‚
â”‚    â–¼ PHASE 5: TARGETED REFINEMENT (IMPROVE pattern)                    â”‚
â”‚    â”‚  â””â”€â”€ Fix weakest DQ dimension per retry                           â”‚
â”‚    â”‚                                                                    â”‚
â”‚    â–¼ PHASE 6: EDITORIAL FRAME                                          â”‚
â”‚    â”‚  â””â”€â”€ Extract thesis / gap / innovation direction                  â”‚
â”‚    â”‚                                                                    â”‚
â”‚    â–¼ Result (DQ score + verifiable citations)                          â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Feature | Description |
|---------|-------------|
| **Tiered Search** | arXiv API + GitHub API + Internal Qdrant |
| **Time-Decay Scoring** | Research: 23-day half-life, News: 2-day |
| **Signal Quantification** | Stars, citations, dates extracted |
| **Grounded Generation** | Agents can ONLY cite retrieved sources |
| **MAR Consensus** | 3 persona critics â†’ synthesis (arXiv:2512.20845) |
| **Targeted Refinement** | IMPROVE pattern (arXiv:2502.18530) |

**Usage:**
```bash
python3 -m cpb precision "your research question" --verbose
```

### v3.5 Changelog

| Feature | Description |
|---------|-------------|
| **Precision Bridge Research** | Tesla US20260017019A1 â†’ RLM synthesis methodology |
| **Cognitive Wallet Tracking** | 114 sessions, 2,530 findings, 8,935 URLs, 27M tokens |
| **Deep Dive Workflow** | Multi-paper synthesis with implementation output |
| **Framework Extraction** | COMPRESS â†’ EXPLORE â†’ RECONSTRUCT pattern identified |

### Notable Research Sessions

| Session | Papers | Output |
|---------|--------|--------|
| Chief of Staff Architecture | 374 | Storage Triad, Graph Intelligence, Writer-Critic |
| Tesla Mixed-Precision RoPE | 15 arXiv | `recursiveLanguageModel.ts` implementation |
| Multi-Agent Orchestration | 12 arXiv | ACE/DQ Scoring in OS-App |
| CPB Integration | 8 arXiv | `cpb/` Python module |
| 160+ Papers Meta-Synthesis | 160+ | Unified research index |

## What's New in v3.4

| Feature | Description |
|---------|-------------|
| **Context Prefetcher** | `prefetch.py` â€” Inject relevant learnings into Claude sessions |
| **Learnings Backfill** | `backfill_learnings.py` â€” Extract learnings from all archived sessions |
| **Memory Injection** | Auto-load project context, papers, and lineage at session start |
| **Shell Integration** | `prefetch`, `prefetch-clip`, `prefetch-inject` shell commands |

### v3.3 Changelog

| Feature | Description |
|---------|-------------|
| **YouTube Research** | `youtube_channel.py` â€” Channel analysis and transcript extraction |
| **Enhanced Backfill** | Improved session recovery with better transcript parsing |
| **Ecosystem Sync** | Deeper integration with Agent Core orchestration |

### v3.2 Changelog

| Feature | Description |
|---------|-------------|
| **Auto-Capture** | Sessions automatically tracked â€” URLs, findings, full transcripts extracted |
| **Lineage Tracking** | Link research sessions to implementation projects |
| **Project Registry** | 4 registered projects with cross-referenced research |
| **Context Loader** | Auto-load project context from any directory |
| **Unified Index** | Cross-reference by paper, topic, or session |
| **Backfill** | Recover research from historical Claude sessions |

---

## Why ResearchGravity?

Traditional research workflows fail at the frontier:

| Problem | Impact |
|---------|--------|
| Single-source blindspots | Missing critical signals |
| No synthesis | Raw links â‰  research |
| No session continuity | Context lost between sessions |
| No quality standard | Inconsistent output |

**ResearchGravity** solves this with:

- **Multi-tier source hierarchy** â€” Tier 1 (primary), Tier 2 (amplifiers), Tier 3 (context)
- **Cold Start Protocol** â€” Never lose session context
- **Synthesis workflow** â€” Thesis â†’ Gap â†’ Innovation Direction
- **Quality checklist** â€” Consistent Metaventions-grade output

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RESEARCHGRAVITY SYSTEM                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    CPB (Cognitive Precision Bridge)                  â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚   Query â”€â”€â†’ Complexity Router â”€â”€â†’ Path Selection â”€â”€â†’ DQ Scoring     â”‚   â”‚
â”‚  â”‚                   â”‚                      â”‚               â”‚           â”‚   â”‚
â”‚  â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”´â”€â”€â”€â”       â”‚   â”‚
â”‚  â”‚           â”‚ Code: +0.25   â”‚      â”‚ ACE 5-Agent â”‚   â”‚ V:40% â”‚       â”‚   â”‚
â”‚  â”‚           â”‚ Reason: +0.20 â”‚      â”‚ Consensus   â”‚   â”‚ S:30% â”‚       â”‚   â”‚
â”‚  â”‚           â”‚ Nav: -0.30    â”‚      â”‚ Engine      â”‚   â”‚ C:30% â”‚       â”‚   â”‚
â”‚  â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                        â”‚                                    â”‚
â”‚                                        â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  SESSION TRACKER  â”‚  â”‚  ROUTING METRICS  â”‚  â”‚  CONFIDENCE SCORER    â”‚   â”‚
â”‚  â”‚  Auto-capture     â”‚  â”‚  DQ history       â”‚  â”‚  Source validation    â”‚   â”‚
â”‚  â”‚  URL logging      â”‚  â”‚  A/B testing      â”‚  â”‚  Evidence scoring     â”‚   â”‚
â”‚  â”‚  Lineage          â”‚  â”‚  Performance      â”‚  â”‚  Quality thresholds   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Directory Structure

```
ResearchGravity/                    # SCRIPTS (git repo)
â”‚
â”œâ”€â”€ repl.py                         # ğŸ†• Interactive REPL (v6.0)
â”œâ”€â”€ auto_capture_v2.py              # ğŸ†• Enhanced auto-capture (v6.0)
â”œâ”€â”€ intelligence.py                 # ğŸ†• CLI intelligence layer (v6.0)
â”œâ”€â”€ watcher.py                      # ğŸ†• File watcher daemon (v6.0)
â”‚
â”œâ”€â”€ cli/                            # ğŸ†• CLI Package (v6.0)
â”‚   â”œâ”€â”€ __init__.py                 # Package exports
â”‚   â”œâ”€â”€ commands.py                 # REPL command handlers
â”‚   â””â”€â”€ ui.py                       # Rich terminal components
â”‚
â”œâ”€â”€ api/                            # REST API Server (v5.0+)
â”‚   â”œâ”€â”€ server.py                   # FastAPI on port 3847 â€” 25 endpoints
â”‚   â””â”€â”€ routes/
â”‚       â””â”€â”€ intelligence.py         # ğŸ†• Intelligence endpoints (v6.0)
â”‚
â”œâ”€â”€ storage/                        # Storage Engine (v5.0+)
â”‚   â”œâ”€â”€ __init__.py                 # Package exports
â”‚   â”œâ”€â”€ sqlite_db.py                # SQLite with WAL mode, FTS5
â”‚   â”œâ”€â”€ sqlite_vec.py               # ğŸ†• sqlite-vec vector storage (v6.0)
â”‚   â”œâ”€â”€ qdrant_db.py                # Vector search (Cohere embeddings)
â”‚   â”œâ”€â”€ engine.py                   # Unified storage interface (dual-write)
â”‚   â”œâ”€â”€ migrate.py                  # JSON â†’ relational migration
â”‚   â”œâ”€â”€ migrate_to_vec.py           # ğŸ†• Qdrant â†’ sqlite-vec migration (v6.0)
â”‚   â””â”€â”€ ucw_ingestion.py            # UCW pack imports
â”‚
â”œâ”€â”€ critic/                         # ğŸ†• Writer-Critic System (v5.0)
â”‚   â”œâ”€â”€ __init__.py                 # Package exports
â”‚   â”œâ”€â”€ base.py                     # CriticBase, ValidationResult, OracleConsensus
â”‚   â”œâ”€â”€ archive_critic.py           # Validates archive completeness
â”‚   â”œâ”€â”€ evidence_critic.py          # Validates citation accuracy
â”‚   â””â”€â”€ pack_critic.py              # Validates context pack relevance
â”‚
â”œâ”€â”€ graph/                          # ğŸ†• Graph Intelligence (v5.0)
â”‚   â”œâ”€â”€ __init__.py                 # Package exports
â”‚   â”œâ”€â”€ lineage.py                  # LineageNode, LineageEdge, LineageGraph
â”‚   â”œâ”€â”€ concept_graph.py            # ConceptGraph â€” relationship traversal
â”‚   â””â”€â”€ queries.py                  # Convenience query functions
â”‚
â”œâ”€â”€ cpb/                            # Cognitive Precision Bridge (v4.0)
â”‚   â”œâ”€â”€ __init__.py                 # Package exports
â”‚   â”œâ”€â”€ types.py                    # Path types, configs, DQScore
â”‚   â”œâ”€â”€ router.py                   # Complexity analysis, path selection
â”‚   â”œâ”€â”€ orchestrator.py             # 5-agent ACE consensus, learning
â”‚   â”œâ”€â”€ dq_scorer.py                # DQ quality measurement
â”‚   â””â”€â”€ cli.py                      # CLI interface
â”‚
â”œâ”€â”€ evidence_extractor.py           # Extract citations from findings
â”œâ”€â”€ evidence_validator.py           # Writer-Critic evidence validation
â”œâ”€â”€ reinvigorate.py                 # Session context reconstruction
â”œâ”€â”€ sync_to_ccc.py                  # CCC dashboard sync
â”œâ”€â”€ prefetch.py                     # Context prefetcher for Claude sessions
â”œâ”€â”€ backfill_learnings.py           # Extract learnings from archived sessions
â”œâ”€â”€ init_session.py                 # Initialize + auto-register sessions
â”œâ”€â”€ session_tracker.py              # Auto-capture engine
â”œâ”€â”€ auto_capture.py                 # Backfill historical sessions
â”œâ”€â”€ archive_session.py              # Archive with critic validation
â”œâ”€â”€ log_url.py                      # Manual URL logging
â”œâ”€â”€ status.py                       # Cold start session checker
â””â”€â”€ SKILL.md                        # Agent Core documentation

~/.agent-core/                      # DATA (single source of truth)
â”œâ”€â”€ projects.json                   # Project registry (v3.2)
â”œâ”€â”€ session_tracker.json            # Auto-capture state
â”œâ”€â”€ research/                       # Project research files
â”‚   â”œâ”€â”€ INDEX.md                    # Unified cross-reference index
â”‚   â”œâ”€â”€ careercoach/
â”‚   â”œâ”€â”€ os-app/
â”‚   â””â”€â”€ metaventions/
â”œâ”€â”€ sessions/                       # Archived sessions
â”‚   â””â”€â”€ [session-id]/
â”‚       â”œâ”€â”€ session.json
â”‚       â”œâ”€â”€ full_transcript.txt
â”‚       â”œâ”€â”€ urls_captured.json
â”‚       â”œâ”€â”€ findings_captured.json
â”‚       â””â”€â”€ lineage.json
â”œâ”€â”€ memory/
â”‚   â”œâ”€â”€ learnings.md                # Extracted learnings archive (v3.4)
â”‚   â”œâ”€â”€ global.md
â”‚   â””â”€â”€ projects/
â””â”€â”€ workflows/

~/.claude/                          # CPB DATA
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ cpb-patterns.jsonl          # CPB execution patterns
â”‚   â””â”€â”€ routing-metrics.jsonl       # Routing performance history
â””â”€â”€ kernel/
    â””â”€â”€ dq-scores.jsonl             # DQ score history
```

---

## CPB Module (v4.0)

The **Cognitive Precision Bridge** provides precision-aware AI orchestration.

### Quick Start

```python
from cpb import cpb, analyze, score_response

# Analyze query complexity
result = analyze("Design a distributed cache system")
print(f"Complexity: {result['complexity_score']:.2f}")
print(f"Path: {result['selected_path']}")

# Build ACE consensus prompts (5 agents)
prompts = cpb.build_ace_prompts("What's the best auth strategy?")
for p in prompts:
    print(f"[{p['agent']}] {p['system_prompt'][:50]}...")

# Score response quality
dq = score_response(query, response)
print(f"DQ: {dq.overall:.2f} (V:{dq.validity:.2f} S:{dq.specificity:.2f} C:{dq.correctness:.2f})")
```

### CLI Commands

```bash
# Analyze query complexity
python3 -m cpb.cli analyze "Your query here"

# Score a response
python3 -m cpb.cli score --query "Q" --response "R"

# View DQ statistics
python3 -m cpb.cli stats --days 30

# Check CPB status
python3 -m cpb.cli status

# Via routing-metrics
python3 routing-metrics.py cpb analyze "Your query"
python3 routing-metrics.py cpb status
```

### ELITE TIER Configuration

| Setting | Value | Description |
|---------|-------|-------------|
| Complexity Thresholds | 0.2 / 0.5 | Lower = more orchestration |
| ACE Agent Count | 5 | Full ensemble |
| DQ Quality Bar | 0.75 | Higher standard |
| Default Path | cascade | Full pipeline |
| RLM Iterations | 25 | Deeper decomposition |
| Model Routing | Opus-first | Maximum quality |

### 5-Agent ACE Ensemble

| Agent | Role | Focus |
|-------|------|-------|
| ğŸ”¬ **Analyst** | Evidence evaluator | Data, logic, consistency |
| ğŸ¤” **Skeptic** | Challenge assumptions | Failure modes, risks |
| ğŸ”„ **Synthesizer** | Pattern finder | Connections, frameworks |
| ğŸ› ï¸ **Pragmatist** | Feasibility checker | Implementation, constraints |
| ğŸ”­ **Visionary** | Strategic thinker | Long-term, second-order effects |

### Research Foundation

- **arXiv:2512.24601** (RLM) - Recursive context externalization
- **arXiv:2511.15755** (DQ) - Decisional quality measurement
- **arXiv:2508.17536** - Voting vs Debate consensus strategies

---

## Installation

### Prerequisites

- Python 3.8+
- pip or pipenv

### Setup

```bash
# Clone the repository
git clone https://github.com/Dicoangelo/ResearchGravity.git
cd ResearchGravity

# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### Configuration

Create `~/.agent-core/config.json` for API keys:

```json
{
  "cohere": {
    "api_key": "your-cohere-api-key"
  }
}
```

Or use environment variables:

```bash
export COHERE_API_KEY="your-cohere-api-key"
```

**Optional (for API server):**

```bash
export RG_SECRET_KEY=$(python -c "import secrets; print(secrets.token_hex(32))")
export RG_API_KEY="your-service-api-key"
```

### Verify Installation

```bash
python3 status.py
```

---

## Quick Start

### 1. Check Session State
```bash
python3 status.py
```

### 2. Initialize New Session
```bash
# Basic session
python3 init_session.py "your research topic"

# Pre-link to implementation project (v3.1)
python3 init_session.py "multi-agent consensus" --impl-project os-app
```

### 3. Research & Log URLs
```bash
# Log a Tier 1 research paper
python3 log_url.py https://arxiv.org/abs/2601.05918 \
  --tier 1 --category research --relevance 5 --used

# Log industry news
python3 log_url.py https://techcrunch.com/... \
  --tier 1 --category industry --relevance 4 --used
```

### 4. Archive When Complete
```bash
python3 archive_session.py
```

### 5. Check Tracker Status (v3.1)
```bash
python3 session_tracker.py status
```

### 6. Load Project Context (v3.2)
```bash
# Auto-detect from current directory
python3 project_context.py

# List all projects
python3 project_context.py --list

# View unified index
python3 project_context.py --index
```

---

## Research Workflow

### Phase 1: Signal Capture (30 min)
```
1. Scan Tier 1 sources for last 24-48 hours
2. Log ALL URLs (used or not) via log_url.py
3. Tag each with: tier, category, relevance (1-5)
```

### Phase 2: Synthesis (20 min)
```
1. Group findings by theme (not source)
2. Identify the GAP â€” what's missing?
3. Draft thesis: "X is happening because Y, which means Z"
```

### Phase 3: Editorial Frame (10 min)
```
1. Write 1-paragraph summary with thesis
2. Each finding: [Name](URL) + signal + rationale
3. End with: "Innovation opportunity: ..."
```

---

## Source Hierarchy

### Tier 1: Primary Sources (Check Daily)

| Category | Sources |
|----------|---------|
| **Research** | arXiv (cs.AI, cs.SE, cs.LG), HuggingFace Papers |
| **Labs** | OpenAI, Anthropic, Google AI, Meta AI, DeepMind |
| **Industry** | TechCrunch, The Verge, Ars Technica, Wired |

### Tier 2: Signal Amplifiers

| Category | Sources |
|----------|---------|
| **GitHub** | Trending, Topics, Releases |
| **Benchmarks** | METR, ARC Prize, LMSYS, PapersWithCode |
| **Social** | X/Twitter key accounts, HN, Reddit ML |

### Tier 3: Deep Context

| Category | Sources |
|----------|---------|
| **Newsletters** | Import AI, The Batch, Latent Space |
| **Forums** | LessWrong, Alignment Forum |

---

## Quality Checklist

Before archiving a session, verify:

- [ ] Scanned all Tier 1 sources for timeframe
- [ ] Logged 10+ URLs minimum
- [ ] Identified at least one GAP
- [ ] Wrote thesis statement
- [ ] Each finding has: link + signal + rationale
- [ ] Innovation direction is concrete, not vague

---

## Cold Start Protocol

When invoking ResearchGravity, always run `status.py` first:

```
==================================================
  ResearchGravity â€” Metaventions AI
==================================================

ğŸ“ ACTIVE SESSION
   Topic: [current topic]
   URLs logged: X | Findings: Y | Thesis: Yes/No

ğŸ“š RECENT SESSIONS
   1. [topic] â€” [date]
   2. [topic] â€” [date]

--------------------------------------------------
OPTIONS:
  â†’ Continue active session
  â†’ Resume archived session
  â†’ Start fresh
--------------------------------------------------
```

---

## Auto-Capture & Lineage (v3.1)

**All research sessions are now automatically tracked.** No more lost research.

### What Gets Captured

| Artifact | Storage |
|----------|---------|
| Full transcript | `~/.agent-core/sessions/[id]/full_transcript.txt` |
| All URLs | `urls_captured.json` |
| Key findings | `findings_captured.json` |
| Cross-project links | `lineage.json` |

### Lineage Tracking

Link research sessions to implementation projects:

```bash
# Pre-link at session start
python3 init_session.py "multi-agent DQ" --impl-project os-app

# Manual link after research
python3 session_tracker.py link [session-id] [project]
```

### Backfill Historical Sessions

Recover research from old Claude sessions:

```bash
# Scan recent history
python3 auto_capture.py scan --hours 48

# Backfill specific session
python3 auto_capture.py backfill ~/.claude/projects/.../session.jsonl --topic "..."
```

---

## Context Prefetcher (v3.4)

**Memory injection for Claude sessions.** Automatically load relevant learnings, project memory, and research papers at session start.

### Basic Usage

```bash
# Auto-detect project from current directory
python3 prefetch.py

# Specific project with papers
python3 prefetch.py --project os-app --papers

# Filter by topic
python3 prefetch.py --topic multi-agent --days 30

# Copy to clipboard
python3 prefetch.py --project os-app --clipboard

# Inject into ~/CLAUDE.md
python3 prefetch.py --project os-app --inject
```

### Shell Commands

After sourcing `~/.claude/scripts/auto-context.sh`:

```bash
prefetch                    # Auto-detect project, last 14 days
prefetch os-app 7           # Specific project, last 7 days
prefetch-clip               # Copy context to clipboard
prefetch-inject             # Inject into ~/CLAUDE.md
prefetch-topic "consensus"  # Filter by topic across all sessions
backfill-learnings          # Regenerate learnings.md from all sessions
```

### CLI Options

| Flag | Description |
|------|-------------|
| `--project, -p` | Project ID to load context for |
| `--topic, -t` | Filter by topic keywords |
| `--days, -d` | Limit to last N days (default: 14) |
| `--limit, -l` | Max learning entries (default: 5) |
| `--papers` | Include relevant arXiv papers |
| `--clipboard, -c` | Copy to clipboard (macOS) |
| `--inject, -i` | Inject into ~/CLAUDE.md |
| `--json` | Output as JSON |
| `--quiet, -q` | Suppress info output |

### Backfill Learnings

Extract learnings from all archived sessions:

```bash
# Process all sessions
python3 backfill_learnings.py

# Last 7 days only
python3 backfill_learnings.py --since 7

# Specific session
python3 backfill_learnings.py --session <session-id>

# Preview without writing
python3 backfill_learnings.py --dry-run
```

### What Gets Injected

| Component | Source |
|-----------|--------|
| Project info | `projects.json` â€” name, focus, tech stack, status |
| Project memory | `memory/projects/[project].md` |
| Recent learnings | `memory/learnings.md` â€” filtered by project/topic/days |
| Research papers | `paper_index` in projects.json |
| Lineage | Research sessions â†’ features implemented |

---

## Integration

ResearchGravity integrates with the **Antigravity ecosystem**:

| Environment | Use Case |
|-------------|----------|
| **CLI** (Claude Code) | Planning, parallel sessions, synthesis |
| **Antigravity** (VSCode) | Coding, preview, browser research |
| **Web** (claude.ai) | Handoff, visual review |

---

## API Server (v5.0)

Start the Chief of Staff API:

```bash
python api/server.py
# Running on http://127.0.0.1:3847
```

### Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/v1/sessions` | GET | List all sessions |
| `/api/v1/sessions/{id}` | GET | Get session details |
| `/api/v1/findings` | GET | Search findings |
| `/api/v1/urls` | GET | Search URLs |
| `/api/v2/graph/stats` | GET | Graph statistics |
| `/api/v2/graph/session/{id}` | GET | Session subgraph (D3 format) |
| `/api/v2/graph/related/{id}` | GET | Related sessions |
| `/api/v2/graph/lineage/{id}` | GET | Research lineage chain |
| `/api/v2/graph/clusters` | GET | Concept clusters |
| `/api/v2/graph/timeline` | GET | Research timeline |
| `/api/v2/graph/network/{id}` | GET | Concept network |
| **`/api/v2/predict/session`** | **POST** | **Predict session outcome, quality, optimal time** |
| **`/api/v2/predict/errors`** | **POST** | **Predict potential errors with solutions** |
| **`/api/v2/predict/optimal-time`** | **POST** | **Suggest best time to work on task** |

### Example Queries

```bash
# Get graph stats
curl http://localhost:3847/api/v2/graph/stats | jq

# Get session subgraph
curl "http://localhost:3847/api/v2/graph/session/my-session-id?depth=2" | jq

# Find concept clusters
curl "http://localhost:3847/api/v2/graph/clusters?min_size=5" | jq

# Predict session outcome (Meta-Learning Engine)
curl -X POST http://localhost:3847/api/v2/predict/session \
  -H "Content-Type: application/json" \
  -d '{"intent": "implement authentication system", "track_prediction": false}' | jq

# Predict potential errors
curl -X POST http://localhost:3847/api/v2/predict/errors \
  -H "Content-Type: application/json" \
  -d '{"intent": "git commit and push", "include_preventable_only": true}' | jq

# Get optimal work time
curl -X POST http://localhost:3847/api/v2/predict/optimal-time \
  -H "Content-Type: application/json" \
  -d '{"intent": "deep architecture work"}' | jq
```

---

## Writer-Critic System (v5.0)

High-stakes outputs are validated by dual-agent critic system:

| Critic | Target | Confidence |
|--------|--------|------------|
| **ArchiveCritic** | Archive completeness (files, metadata, findings) | 96.3% |
| **EvidenceCritic** | Citation accuracy, source validation | Threshold: 0.7 |
| **PackCritic** | Context pack relevance, token efficiency | Threshold: 0.7 |

```python
from critic import ArchiveCritic, EvidenceCritic

# Validate an archive
critic = ArchiveCritic()
result = await critic.validate("session-id")
print(f"Valid: {result.valid}, Confidence: {result.confidence:.2%}")
```

---

## Graph Intelligence (v5.0)

Query the knowledge graph:

```python
from graph import ConceptGraph, get_research_lineage

# Get session subgraph
graph = ConceptGraph()
await graph.load()
subgraph = await graph.get_session_graph("session-id", depth=2)
d3_data = subgraph.to_d3_format()  # For visualization

# Get research lineage
lineage = await get_research_lineage("session-id")
print(f"Ancestors: {len(lineage['ancestors'])}")
print(f"Descendants: {len(lineage['descendants'])}")

# Find concept clusters
clusters = await graph.get_concept_clusters(min_size=5)
```

---

## Roadmap

### Completed âœ…
- [x] Auto-capture sessions (v3.1)
- [x] Cross-project lineage tracking (v3.1)
- [x] Project registry & context loader (v3.2)
- [x] Unified research index (v3.2)
- [x] Context prefetcher & memory injection (v3.4)
- [x] Learnings backfill from archived sessions (v3.4)
- [x] CPB â€” Cognitive Precision Bridge (v4.0)
- [x] Storage Triad â€” SQLite + Qdrant (v5.0)
- [x] Writer-Critic validation system (v5.0)
- [x] Graph Intelligence â€” concept relationships (v5.0)
- [x] REST API â€” 19 endpoints (v5.0)
- [x] Evidence Layer â€” citations & confidence (v5.0)
- [x] CCC Dashboard sync (v5.0)
- [x] Interactive REPL â€” real-time research CLI (v6.0)
- [x] Auto-Capture V2 â€” +70% URL capture rate (v6.0)
- [x] Intelligence Layer â€” CLI + API + REPL (v6.0)
- [x] sqlite-vec storage â€” offline vector search (v6.0)
- [x] File Watcher â€” implicit session creation (v6.0)
- [x] Dual-Write Engine â€” Qdrant + sqlite-vec failover (v6.0)

### Future
- [ ] OS-App SDK integration
- [ ] Real-time WebSocket updates
- [ ] Browser extension for URL capture
- [ ] Team collaboration features

---

## License

MIT License â€” See [LICENSE](LICENSE)

---

## Contact

**Metaventions AI**
Dico Angelo
dicoangelo@metaventionsai.com

<p align="center">
  <a href="https://metaventionsai.com">
    <img src="https://img.shields.io/badge/Metaventions_AI-Website-00d9ff?style=for-the-badge" alt="Website" />
  </a>
  <a href="https://github.com/Dicoangelo">
    <img src="https://img.shields.io/badge/GitHub-Dicoangelo-1a1a2e?style=for-the-badge&logo=github" alt="GitHub" />
  </a>
</p>

<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=0:1a1a2e,100:00d9ff&height=100&section=footer" />
</p>
